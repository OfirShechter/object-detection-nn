{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dcor/niskhizov/Rar'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dcor/niskhizov/Rar/object-detection-nn\n"
     ]
    }
   ],
   "source": [
    "cd object-detection-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ofir1\\Msc\\object-detection-nn\n"
     ]
    }
   ],
   "source": [
    "# cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dcor/niskhizov/anaconda3/envs/obj_d/lib/python3.11/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "remote_mode = True\n",
    "\n",
    "import os\n",
    "\n",
    "from nn.YOLO_VGG16.utils.constants import ANCHORS\n",
    "from nn.YOLO_VGG16.prepare_data.coco_dataset import CocoDataset\n",
    "from nn.YOLO_VGG16.prepare_data.transforms import train_transform, test_transform\n",
    "from nn.YOLO_VGG16.utils.helpers import convert_cells_to_bboxes, load_checkpoint, nms, plot_image, save_checkpoint\n",
    "from nn.YOLO_VGG16.utils.constants import device, s, leanring_rate, save_model, checkpoint_file\n",
    "from nn.YOLO_VGG16.model.YOLO_VGG16_full import YOLO_VGG16_F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from nn.YOLO_VGG16.model.loss import YOLOLoss\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as T\n",
    "\n",
    "if remote_mode:\n",
    "    model_path_base = f\"/home/dcor/niskhizov/Rar/object-detection-nn/nn/YOLO_VGG16/degug_notebooks/vgg_f_airplain_model\"\n",
    "    coco_path = lambda type: f'/home/dcor/niskhizov/Rar/object-detection-nn/nn/YOLO_VGG16/degug_notebooks/temp/instances_{type}2017.json'\n",
    "else:\n",
    "    model_path_base = f\"nn/YOLO_VGG16/degug_notebooks/\"\n",
    "    coco_path = lambda type: f'nn/cocodataset/annotations/instances_{type}2017.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=16.54s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO(coco_path('train'))\n",
    "categories = ['person',\n",
    " 'bicycle',\n",
    " 'car',\n",
    " 'motorcycle',\n",
    " 'airplane',\n",
    " 'bus',\n",
    " 'train',\n",
    " 'truck',\n",
    " 'boat',\n",
    " 'traffic light',\n",
    " 'fire hydrant',\n",
    " 'stop sign',\n",
    " 'parking meter',\n",
    " 'bench',\n",
    " 'bird',\n",
    " 'cat',\n",
    " 'dog',\n",
    " 'horse',\n",
    " 'sheep',\n",
    " 'cow',\n",
    " 'elephant',\n",
    " 'bear',\n",
    " 'zebra',\n",
    " 'giraffe',\n",
    " 'backpack',\n",
    " 'umbrella',\n",
    " 'handbag',\n",
    " 'tie',\n",
    " 'suitcase',\n",
    " 'frisbee',\n",
    " 'skis',\n",
    " 'snowboard',\n",
    " 'sports ball',\n",
    " 'kite',\n",
    " 'baseball bat',\n",
    " 'baseball glove',\n",
    " 'skateboard',\n",
    " 'surfboard',\n",
    " 'tennis racket',\n",
    " 'bottle',\n",
    " 'wine glass',\n",
    " 'cup',\n",
    " 'fork',\n",
    " 'knife',\n",
    " 'spoon',\n",
    " 'bowl',\n",
    " 'banana',\n",
    " 'apple',\n",
    " 'sandwich',\n",
    " 'orange',\n",
    " 'broccoli',\n",
    " 'carrot',\n",
    " 'hot dog',\n",
    " 'pizza',\n",
    " 'donut',\n",
    " 'cake',\n",
    " 'chair',\n",
    " 'couch',\n",
    " 'potted plant',\n",
    " 'bed',\n",
    " 'dining table',\n",
    " 'toilet',\n",
    " 'tv',\n",
    " 'laptop',\n",
    " 'mouse',\n",
    " 'remote',\n",
    " 'keyboard',\n",
    " 'cell phone',\n",
    " 'microwave',\n",
    " 'oven',\n",
    " 'toaster',\n",
    " 'sink',\n",
    " 'refrigerator',\n",
    " 'book',\n",
    " 'clock',\n",
    " 'vase',\n",
    " 'scissors',\n",
    " 'teddy bear',\n",
    " 'hair drier',\n",
    " 'toothbrush'] # 80 classes of COCO dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ids = coco.getCatIds(catNms=categories)\n",
    "# Get image ids for each category- should work with coco.getImgIds(catIds=cat_ids), but has a bug\n",
    "img_ids = [coco.getImgIds(\n",
    "    catIds=cat_id) for cat_id in cat_ids]\n",
    "cat_ids_map = {cat_id: i for i, cat_id in enumerate(cat_ids)}\n",
    "# Flatten the list\n",
    "img_ids = [item for sublist in img_ids for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342996"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model from YOLOv3 class \n",
    "load_model = False\n",
    "model = YOLO_VGG16_F(num_classes=len(categories)).to(device) \n",
    "\n",
    "# Defining the optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr = leanring_rate) \n",
    "\n",
    "# Defining the loss function \n",
    "loss_fn = YOLOLoss() \n",
    "\n",
    "# Defining the scaler for mixed precision training \n",
    "scaler = torch.amp.GradScaler(device=device) \n",
    "# Loading the checkpoint \n",
    "if load_model: \n",
    "    load_checkpoint(model_path_base + f\"e49_vgg16_{checkpoint_file}\", model, optimizer, leanring_rate, device) \n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='runs/YOLO_VGG16_F_many_classes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CocoDataset( \n",
    "\tcoco_obj=coco, \n",
    "\tcategories=categories,\n",
    "\tgrid_sizes=[13, 26, 52], \n",
    "\tanchors=ANCHORS, \n",
    "\ttransform=train_transform \n",
    ") \n",
    "\n",
    "# Defining the train data loader \n",
    "train_loader = torch.utils.data.DataLoader( \n",
    "\tdataset=dataset, \n",
    "\tbatch_size=8, \n",
    "\tshuffle=True, \n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Load the validation dataset\n",
    "val_coco = COCO(coco_path('val'))\n",
    "\n",
    "val_dataset = CocoDataset(\n",
    "    coco_obj=val_coco,\n",
    "    categories=categories,\n",
    "    grid_sizes=[13, 26, 52],\n",
    "    anchors=ANCHORS,\n",
    "    transform=test_transform  # Use the same transform for validation\n",
    ")\n",
    "\n",
    "# Create the validation data loader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the anchors \n",
    "scaled_anchors = ( \n",
    "\ttorch.tensor(ANCHORS) *\n",
    "\ttorch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1,3,2) \n",
    ").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                        | 0/42875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                             | 0/42875 [00:09<?, ?it/s, loss=25.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 2/42875 [00:20<114:39:30,  9.63s/it, loss=25.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 1\n",
      "batch_idx: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 4/42875 [00:34<96:20:26,  8.09s/it, loss=25.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 5/42875 [00:41<91:54:20,  7.72s/it, loss=25.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 6/42875 [00:49<90:35:43,  7.61s/it, loss=25.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 7/42875 [00:56<90:44:41,  7.62s/it, loss=25.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 8/42875 [01:04<89:41:41,  7.53s/it, loss=25.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 9/42875 [01:12<91:41:58,  7.70s/it, loss=25.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 10/42875 [01:19<89:14:53,  7.50s/it, loss=25.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 11/42875 [01:26<87:13:30,  7.33s/it, loss=25.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 12/42875 [01:33<86:45:53,  7.29s/it, loss=25.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 13/42875 [01:40<87:11:48,  7.32s/it, loss=25.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 14/42875 [01:48<88:27:43,  7.43s/it, loss=25.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 15/42875 [01:55<87:39:58,  7.36s/it, loss=25.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 16/42875 [02:03<88:40:40,  7.45s/it, loss=25.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 17/42875 [02:10<87:44:25,  7.37s/it, loss=25.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 18/42875 [02:18<89:17:28,  7.50s/it, loss=25.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 19/42875 [02:25<88:24:03,  7.43s/it, loss=25.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 20/42875 [02:33<88:47:40,  7.46s/it, loss=25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 21/42875 [02:39<85:32:53,  7.19s/it, loss=25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                   | 22/42875 [02:47<85:53:28,  7.22s/it, loss=25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 23/42875 [02:54<85:21:09,  7.17s/it, loss=24.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 24/42875 [03:01<86:07:17,  7.24s/it, loss=24.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 23\n"
     ]
    }
   ],
   "source": [
    "epochs = 100000000\n",
    "# Training the model \n",
    "for e in range(1, epochs+1): \n",
    "\tprint(\"Epoch:\", e) \n",
    "    ################# dos\n",
    "    # Creating a progress bar \n",
    "\tprogress_bar = tqdm(train_loader, leave=True) \n",
    "\n",
    "\t# Initializing a list to store the losses \n",
    "\tlosses = [] \n",
    "\n",
    "\t# Iterating over the training data \n",
    "\tfor batch_idx, (x, y) in enumerate(progress_bar): \n",
    "\t\tprint(\"batch_idx:\", batch_idx)\n",
    "\t\tx = x.to(device) \n",
    "\t\ty0, y1, y2 = ( \n",
    "\t\t\ty[0].to(device), \n",
    "\t\t\ty[1].to(device), \n",
    "\t\t\ty[2].to(device), \n",
    "\t\t) \n",
    "\n",
    "\t\twith torch.amp.autocast(device_type=device): \n",
    "\t\t\t# Getting the model predictions \n",
    "\t\t\toutputs = model(x) \n",
    "\t\t\t# Calculating the loss at each scale \n",
    "\t\t\tloss = ( \n",
    "\t\t\t\tloss_fn(outputs[0], y0, scaled_anchors[0]) \n",
    "\t\t\t\t+ loss_fn(outputs[1], y1, scaled_anchors[1]) \n",
    "\t\t\t\t+ loss_fn(outputs[2], y2, scaled_anchors[2]) \n",
    "\t\t\t) \n",
    "\n",
    "\t\t# Add the loss to the list \n",
    "\t\tlosses.append(loss.item()) \n",
    "\n",
    "\t\t# Reset gradients \n",
    "\t\toptimizer.zero_grad() \n",
    "\n",
    "\t\t# Backpropagate the loss \n",
    "\t\tscaler.scale(loss).backward() \n",
    "\n",
    "\t\t# Optimization step \n",
    "\t\tscaler.step(optimizer) \n",
    "\n",
    "\t\t# Update the scaler for next iteration \n",
    "\t\tscaler.update() \n",
    "\n",
    "\t\t# update progress bar with loss \n",
    "\t\tmean_loss = sum(losses) / len(losses) \n",
    "\t\tprogress_bar.set_postfix(loss=mean_loss)\n",
    "  \n",
    "        # Log the loss to TensorBoard\n",
    "\t\twriter.add_scalar('Loss/train', mean_loss, e * len(train_loader) + batch_idx)\n",
    "\n",
    "\t\t# Log images to TensorBoard every 100 batches\n",
    "\t\tif batch_idx % 200 == 0:\n",
    "\t\t\t# Saving the model \n",
    "\t\t\tif save_model: \n",
    "\t\t\t\tsave_checkpoint(model, optimizer, filename=model_path_base +f\"b{batch_idx}_vgg16_checkpoint.pth.tar\")\n",
    "\n",
    "   \n",
    "\t# Saving the model \n",
    "\tif save_model: \n",
    "\t\tsave_checkpoint(model, optimizer, filename=model_path_base +f\"e{e}_vgg16_checkpoint.pth.tar\")\n",
    "        # delete batch checkpoints\n",
    "\t\tfor i in range(0, batch_idx+1, 200):\n",
    "\t\t\tos.remove(model_path_base + f\"b{i}_vgg16_checkpoint.pth.tar\")\n",
    "\t\t# for i in range(0,e-3):\n",
    "\t\t# \tif os.path.exists(model_path_base + f\"e{i}_vgg16_checkpoint.pth.tar\"):\n",
    "\t\t# \t\tos.remove(model_path_base + f\"e{i}_vgg16_checkpoint.pth.tar\")\n",
    "   \n",
    "\tmodel.eval()\n",
    " \n",
    "\t# Getting a sample image from the test data loader \n",
    "\ttry:\n",
    "\t\tx, y = next(val_loader_iter)\n",
    "\texcept StopIteration:\n",
    "\t\tval_loader_iter = iter(val_loader)\n",
    "\t\tx, y = next(val_loader_iter)\n",
    "\tx = x.to(device) \n",
    "\t\n",
    "\tprint(\"display and report image\")\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(x)\n",
    "\t\ty0, y1, y2 = ( \n",
    "\t\t\ty[0].to(device), \n",
    "\t\t\ty[1].to(device), \n",
    "\t\t\ty[2].to(device), \n",
    "\t\t) \n",
    "\n",
    "\t\twith torch.amp.autocast(device_type=device): \n",
    "\t\t\t# Getting the model predictions \n",
    "\t\t\toutputs = model(x) \n",
    "\t\t\t# Calculating the loss at each scale \n",
    "\t\t\tloss = ( \n",
    "\t\t\t\tloss_fn(outputs[0], y0, scaled_anchors[0]) \n",
    "\t\t\t\t+ loss_fn(outputs[1], y1, scaled_anchors[1]) \n",
    "\t\t\t\t+ loss_fn(outputs[2], y2, scaled_anchors[2]) \n",
    "\t\t\t) \n",
    "   \n",
    "\t\twriter.add_scalar('Loss/val', loss.item(), e)\n",
    "  \n",
    "\t\tbboxes = [[] for _ in range(x.shape[0])]\n",
    "\t\tfor i in range(3):\n",
    "\t\t\tbatch_size, A, S, _, _ = output[i].shape\n",
    "\t\t\tanchor = scaled_anchors[i]\n",
    "\t\t\tboxes_scale_i = convert_cells_to_bboxes(output[i], anchor, s=S, is_predictions=True)\n",
    "\t\t\tfor idx, box in enumerate(boxes_scale_i):\n",
    "\t\t\t\tbboxes[idx] += box\n",
    "\t\ti = 0\n",
    "\t\tnms_boxes = nms(bboxes[i], iou_threshold=0.5, threshold=0.6)\n",
    "\t\timg_with_boxes = plot_image(x[i].permute(1, 2, 0).detach().cpu(), nms_boxes, categories)\n",
    "\t\timg_with_boxes = T.ToTensor()(img_with_boxes)\n",
    "\t\twriter.add_image(f'Val/Image_{e}_{i}_before', img_with_boxes, e * len(train_loader) + batch_idx)\n",
    "\n",
    "\tmodel.train()\n",
    "\n",
    "\n",
    "    #################\n",
    "\t# training_loop(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obj_d",
   "language": "python",
   "name": "obj_d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
