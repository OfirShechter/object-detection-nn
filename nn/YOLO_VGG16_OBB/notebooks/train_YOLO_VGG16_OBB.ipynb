{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd object-detection-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_mode = True\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from nn.YOLO_VGG16_OBB.utils.constants import ANCHORS\n",
    "from nn.YOLO_VGG16_OBB.prepare_data.dota_dataset_memory import DotaDataset\n",
    "from nn.YOLO_VGG16_OBB.prepare_data.transforms import train_transform, test_transform\n",
    "from nn.YOLO_VGG16_OBB.utils.helpers import convert_cells_to_bboxes, load_checkpoint, nms, plot_image, save_checkpoint\n",
    "from nn.YOLO_VGG16_OBB.utils.constants import device, s, leanring_rate, save_model, checkpoint_file\n",
    "from nn.YOLO_VGG16_OBB.model.YOLO_VGG16_OBB import YOLO_VGG16_OBB\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from nn.YOLO_VGG16_OBB.model.loss import YOLOLoss\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as T\n",
    "\n",
    "if remote_mode:\n",
    "    model_path_base = f\"/home/dcor/niskhizov/Rar/object-detection-nn/nn/YOLO_VGG16_OBB/notebooks/vgg_f_obb_v2_model\"\n",
    "else:\n",
    "    model_path_base = f\"nn/YOLO_VGG16_OBB/notebooks/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['plane','ship', 'storage-tank', 'baseball-diamond', 'tennis-court', 'basketball-court', 'ground-track-field', 'harbor', 'bridge', 'large-vehicle', 'small-vehicle', 'helicopter', 'roundabout', 'soccer-ball-field', 'swimming-pool']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model from YOLOv3 class \n",
    "load_model = True\n",
    "save_model = True\n",
    "model = YOLO_VGG16_OBB(num_classes=len(categories)).to(device) \n",
    "\n",
    "# Defining the optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr = leanring_rate) \n",
    "\n",
    "# Defining the loss function \n",
    "loss_fn = YOLOLoss() \n",
    "\n",
    "# Defining the scaler for mixed precision training \n",
    "scaler = torch.amp.GradScaler(device=device) \n",
    "# Loading the checkpoint \n",
    "if load_model: \n",
    "    load_checkpoint(model_path_base + f\"e276_vgg16_{checkpoint_file}\", model, optimizer, leanring_rate, device) \n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='runs/YOLO_VGG16_OBB_complete_v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DotaDataset( \n",
    "\tcategories=categories,\n",
    "\tgrid_sizes=[13, 26, 52], \n",
    "\tanchors=ANCHORS, \n",
    "\ttransform=train_transform \n",
    ") \n",
    "\n",
    "# Defining the train data loader \n",
    "train_loader = torch.utils.data.DataLoader( \n",
    "\tdataset=dataset, \n",
    "\tbatch_size=8, \n",
    "\tshuffle=True, \n",
    "\tnum_workers=4,\n",
    " \tprefetch_factor=10,\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = DotaDataset(\n",
    "    categories=categories,\n",
    "    grid_sizes=[13, 26, 52],\n",
    "    anchors=ANCHORS,\n",
    "    transform=test_transform,  # Use the same transform for validation\n",
    "    data_base_path = f\"nn/dotadataset/val\"\n",
    ")\n",
    "\n",
    "# Create the validation data loader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_loader_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the anchors \n",
    "scaled_anchors = ( \n",
    "\ttorch.tensor(ANCHORS) *\n",
    "\ttorch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1,3,2) \n",
    ").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = YOLOLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(model, optimizer, filename=model_path_base +f\"e{e}_vgg16_checkpoint.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100000000\n",
    "error_counter = 0\n",
    "save_model = True\n",
    "# Training the model \n",
    "for e in range(297, epochs+1): \n",
    "\tprint(\"Epoch:\", e) \n",
    "    ################# dos\n",
    "    # Creating a progress bar \n",
    "\tprogress_bar = tqdm(train_loader, leave=True) \n",
    "\n",
    "\t# Initializing a list to store the losses \n",
    "\tlosses = [] \n",
    "\t# try:\n",
    "\t\t# Iterating over the training data \n",
    "\tfor batch_idx, (x, y) in enumerate(progress_bar):\n",
    "\t\t\t# try:\n",
    "\t\t\t\tprint(\"batch_idx:\", batch_idx)\n",
    "\t\t\t\tx = x.to(device) \n",
    "\t\t\t\ty0, y1, y2 = ( \n",
    "\t\t\t\t\ty[0].to(device), \n",
    "\t\t\t\t\ty[1].to(device), \n",
    "\t\t\t\t\ty[2].to(device), \n",
    "\t\t\t\t) \n",
    "\n",
    "\t\t\t\twith torch.amp.autocast(device_type=device): \n",
    "\t\t\t\t\t# Getting the model predictions \n",
    "\t\t\t\t\toutputs = model(x) \n",
    "\t\t\t\t\t# Calculating the loss at each scale \n",
    "\t\t\t\t\tloss = ( \n",
    "\t\t\t\t\t\tloss_fn(outputs[0], y0, scaled_anchors[0]) \n",
    "\t\t\t\t\t\t+ loss_fn(outputs[1], y1, scaled_anchors[1]) \n",
    "\t\t\t\t\t\t+ loss_fn(outputs[2], y2, scaled_anchors[2]) \n",
    "\t\t\t\t\t) \n",
    "\n",
    "\t\t\t\t# Add the loss to the list \n",
    "\t\t\t\tlosses.append(loss.item()) \n",
    "\n",
    "\t\t\t\t# Reset gradients \n",
    "\t\t\t\toptimizer.zero_grad() \n",
    "\n",
    "\t\t\t\t# Backpropagate the loss \n",
    "\t\t\t\tscaler.scale(loss).backward() \n",
    "\n",
    "\t\t\t\t# Optimization step \n",
    "\t\t\t\tscaler.step(optimizer) \n",
    "\n",
    "\t\t\t\t# Update the scaler for next iteration \n",
    "\t\t\t\tscaler.update() \n",
    "\n",
    "\t\t\t\t# update progress bar with loss \n",
    "\t\t\t\tmean_loss = sum(losses) / len(losses) \n",
    "\t\t\t\tprogress_bar.set_postfix(loss=mean_loss)\n",
    "\t\t\n",
    "\t\t\t\t# Log the loss to TensorBoard\n",
    "\t\t\t\twriter.add_scalar('Loss/train', mean_loss, e * len(train_loader) + batch_idx)\n",
    "\n",
    "\t\t\t\t# # Log images to TensorBoard every 100 batches\n",
    "\t\t\t\t# if batch_idx % 200 == 0:\n",
    "\t\t\t\t# \t# Saving the model \n",
    "\t\t\t\t# \tif save_model: \n",
    "\t\t\t\t# \t\tsave_checkpoint(model, optimizer, filename=model_path_base +f\"b{batch_idx}_vgg16_checkpoint.pth.tar\")\n",
    "\t\t\n",
    "\t\t\t\t# \tfor i in range(0, batch_idx-3, 200):\n",
    "\t\t\t\t# \t\t\tif os.path.exists(model_path_base + f\"b{i}_vgg16_checkpoint.pth.tar\"):\n",
    "\t\t\t\t# \t\t\t\tos.remove(model_path_base + f\"b{i}_vgg16_checkpoint.pth.tar\")\n",
    "\n",
    "\tif e % 2 == 0:\n",
    "\t\t# Saving the model \n",
    "\t\tif save_model: \n",
    "\t\t\tsave_checkpoint(model, optimizer, filename=model_path_base +f\"e{e}_vgg16_checkpoint.pth.tar\")\n",
    "\t\t\t# # delete batch checkpoints\n",
    "\t\t\t# for i in range(0, e-2):\n",
    "\t\t\t# \tif os.path.exists(model_path_base + f\"e{e}_vgg16_checkpoint.pth.tar\"):\n",
    "\t\t\t# \t\tos.remove(model_path_base + f\"e{e}_vgg16_checkpoint.pth.tar\")\n",
    "\t\t\t# for i in range(0,e-3):\n",
    "\t\t\t# \tif os.path.exists(model_path_base + f\"e{i}_vgg16_checkpoint.pth.tar\"):\n",
    "\t\t\t# \t\tos.remove(model_path_base + f\"e{i}_vgg16_checkpoint.pth.tar\")\n",
    "\t\n",
    "\t\t# model.eval()\n",
    "\t\n",
    "\t\t# # Getting a sample image from the test data loader \n",
    "\t\t# try:\n",
    "\t\t# \tx, y = next(val_loader_iter)\n",
    "\t\t# except StopIteration:\n",
    "\t\t# \tval_loader_iter = iter(val_loader)\n",
    "\t\t# \tx, y = next(val_loader_iter)\n",
    "\t\t# x = x.to(device) \n",
    "\t\t\n",
    "\t\t# print(\"###################################### display and report image ######################################\")\n",
    "\t\t# with torch.no_grad():\n",
    "\t\t# \tscaled_anchors = ( \n",
    "\t\t# \ttorch.tensor(ANCHORS) *\n",
    "\t\t# \ttorch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1,3,2) \n",
    "\t\t# \t).to(device) \n",
    "\t\t# \toutput = model(x)\n",
    "\t\t# \ty0, y1, y2 = ( \n",
    "\t\t# \t\ty[0].to(device), \n",
    "\t\t# \t\ty[1].to(device), \n",
    "\t\t# \t\ty[2].to(device), \n",
    "\t\t# \t) \n",
    "\n",
    "\t\t# \twith torch.amp.autocast(device_type=device): \n",
    "\t\t# \t\t# Getting the model predictions \n",
    "\t\t# \t\toutputs = model(x) \n",
    "\t\t# \t\t# Calculating the loss at each scale \n",
    "\t\t# \t\tloss = ( \n",
    "\t\t# \t\t\tloss_fn(outputs[0], y0, scaled_anchors[0]) \n",
    "\t\t# \t\t\t+ loss_fn(outputs[1], y1, scaled_anchors[1]) \n",
    "\t\t# \t\t\t+ loss_fn(outputs[2], y2, scaled_anchors[2]) \n",
    "\t\t# \t\t) \n",
    "\t\n",
    "\t\t# \t# Print predictions\n",
    "\t\t# \twriter.add_scalar('Loss/val', loss.item(), e * len(train_loader) + batch_idx)\n",
    "\n",
    "\t\t\t# bboxes = [[] for _ in range(x.shape[0])]\n",
    "\t\t\t# for i in range(3):\n",
    "\t\t\t# \tbatch_size, A, S, _, _ = output[i].shape\n",
    "\t\t\t# \tanchor = scaled_anchors[i]\n",
    "\t\t\t# \tboxes_scale_i = convert_cells_to_bboxes(output[i], anchor, s=S, is_predictions=True)\n",
    "\t\t\t# \tfor idx, box in enumerate(boxes_scale_i):\n",
    "\t\t\t# \t\tbboxes[idx] += box\n",
    "\t\n",
    "\t\t\t# i = 0\n",
    "\t\t\t# print('bboxes[i] shape:', np.array(bboxes[i]).shape)\n",
    "\t\t\t# nms_boxes = nms(bboxes[i], iou_threshold=0.5, threshold=0.6)\n",
    "\t\t\t# img_with_boxes = plot_image(x[i].permute(1, 2, 0).detach().cpu(), nms_boxes, categories)\n",
    "\t\t\t# img_with_boxes = T.ToTensor()(img_with_boxes)\n",
    "\t\t\t# writer.add_image(f'Val/Image_{e}_{i}_{batch_idx}_before', img_with_boxes, e * len(train_loader) + batch_idx)\n",
    "\n",
    "\t\tmodel.train()\n",
    "\t# except Exception as error:\n",
    "\t# \tprint(error)\n",
    "\t# \terror_counter += 1\n",
    "\t# \tif error_counter > 10:\n",
    "\t# \t\traise error\n",
    "\n",
    "\n",
    "    #################\n",
    "\t# training_loop(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([(pred[..., 1:3]),\n",
    "                        torch.exp(pred[..., 3:5]) *\n",
    "                        anchors, pred_angle.unsqueeze(-1)\n",
    "                        ], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[..., 1:3].shape, pred[..., 3:5].shape, pred_angle.unsqueeze(-1).shape, anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = outputs[1]\n",
    "target = y1 \n",
    "anchors = scaled_anchors[1]\n",
    "pred_angle = torch.tanh(pred[..., 5])\n",
    "box_preds = torch.cat([(pred[..., 1:3]),\n",
    "                        torch.exp(pred[..., 3:5]) *\n",
    "                        anchors, pred_angle.unsqueeze(-1)\n",
    "                        ], dim=-1)\n",
    "# Calculating intersection over union for prediction and target\n",
    "ious = skew_iou(box_preds[obj], target[..., 1:6][obj]).detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def get_rotated_box_corners(boxes):\n",
    "    \"\"\"\n",
    "    Convert rotated bounding boxes [x, y, w, h, theta] into their four corner points.\n",
    "    \n",
    "    Args:\n",
    "    - boxes: (N, 5) Tensor containing (x, y, w, h, theta) in radians.\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor of shape (N, 4, 2) representing the four corner points of each box.\n",
    "    \"\"\"\n",
    "    x, y, w, h, theta = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3], boxes[:, 4]\n",
    "    \n",
    "    # Compute half dimensions\n",
    "    w_half, h_half = w / 2, h / 2\n",
    "\n",
    "    # Define corner points relative to the center\n",
    "    corners = torch.tensor([\n",
    "        [-1, -1],\n",
    "        [ 1, -1],\n",
    "        [ 1,  1],\n",
    "        [-1,  1]\n",
    "    ], dtype=boxes.dtype, device=boxes.device)  # Shape (4, 2)\n",
    "\n",
    "    # Scale corners by (w/2, h/2)\n",
    "    scaled_corners = corners * torch.stack([w_half, h_half], dim=-1).unsqueeze(1)  # (N, 4, 2)\n",
    "\n",
    "    # Compute rotation matrix\n",
    "    cos_t, sin_t = torch.cos(theta), torch.sin(theta)\n",
    "    rotation_matrix = torch.stack([\n",
    "        torch.stack([cos_t, -sin_t], dim=-1),\n",
    "        torch.stack([sin_t, cos_t], dim=-1)\n",
    "    ], dim=-2)  # Shape (N, 2, 2)\n",
    "\n",
    "    # Rotate corners and translate to (x, y)\n",
    "    rotated_corners = torch.bmm(scaled_corners, rotation_matrix) + torch.stack([x, y], dim=-1).unsqueeze(1)\n",
    "    \n",
    "    return rotated_corners  # Shape (N, 4, 2)\n",
    "\n",
    "\n",
    "def skew_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Compute IoU between batches of rotated bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "    - boxes1: (N, 5) Tensor (x, y, w, h, theta) in radians.\n",
    "    - boxes2: (N, 5) Tensor (x, y, w, h, theta) in radians.\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor (N,) containing IoU values for each pair.\n",
    "    \"\"\"\n",
    "    N = boxes1.shape[0]\n",
    "    assert boxes2.shape[0] == N, \"Batch sizes must match\"\n",
    "\n",
    "    # Get corners for each box\n",
    "    corners1 = get_rotated_box_corners(boxes1)\n",
    "    corners2 = get_rotated_box_corners(boxes2)\n",
    "    corners1 = corners1.detach().cpu().numpy()\n",
    "    corners2 = corners2.detach().cpu().numpy()\n",
    "    # Compute IoU using Shapely\n",
    "    ious = []\n",
    "    for i in range(N):\n",
    "        poly1 = Polygon(corners1[i])\n",
    "        poly2 = Polygon(corners2[i])\n",
    "        \n",
    "        if not poly1.is_valid or not poly2.is_valid:\n",
    "            ious.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        inter_area = poly1.intersection(poly2).area\n",
    "        union_area = poly1.area + poly2.area - inter_area\n",
    "\n",
    "        ious.append(inter_area / union_area if union_area > 0 else 0.0)\n",
    "\n",
    "    return torch.tensor(ious, dtype=boxes1.dtype, device=boxes1.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from nn.YOLO_VGG16_OBB.utils.helpers import iou\n",
    "\n",
    "# Defining YOLO loss class\n",
    "\n",
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.than = nn.Tanh()\n",
    "\n",
    "    def forward(self, pred, target, anchors):\n",
    "        # Identifying which cells in target have objects\n",
    "        # and which have no objects\n",
    "        obj = target[..., 0] == 1\n",
    "        no_obj = target[..., 0] == 0\n",
    "\n",
    "        # Calculating No object loss\n",
    "        no_object_loss = self.bce(\n",
    "            self.sigmoid(pred[..., 0:1][no_obj]), (target[..., 0:1][no_obj]),\n",
    "        )\n",
    "        \n",
    "        # reg_object_loss = 10 * self.bce(\n",
    "        #     self.sigmoid(pred[..., 0:1][obj]), (target[..., 0:1][obj]),\n",
    "        # )\n",
    "\n",
    "        # Reshaping anchors to match predictions\n",
    "        anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
    "        pred_angle = torch.tanh(pred[..., 5])\n",
    "        # Box prediction confidence\n",
    "        box_preds = torch.cat([self.sigmoid(pred[..., 1:3]),\n",
    "                               torch.exp(pred[..., 3:5]) *\n",
    "                               anchors, pred_angle.unsqueeze(-1)\n",
    "                               ], dim=-1)\n",
    "        # Calculating intersection over union for prediction and target\n",
    "        ious = iou(box_preds[obj], target[..., 1:6][obj]).detach()\n",
    "        # Calculating Object loss\n",
    "        object_loss = self.bce(self.sigmoid(pred[..., 0:1][obj]),\n",
    "                               ious * target[..., 0:1][obj])\n",
    "\n",
    "        # Predicted box coordinates\n",
    "        pred[..., 1:3] = self.sigmoid(pred[..., 1:3])\n",
    "        # Target box coordinates\n",
    "        target[..., 3:5] = torch.log(1e-6 + target[..., 3:5] / anchors)\n",
    "        # Calculating box coordinate loss\n",
    "\n",
    "        box_loss = self.mse(pred[..., 1:5][obj],\n",
    "                            target[..., 1:5][obj])\n",
    "        \n",
    "        box_loss_center = self.mse(pred[..., 1:3][obj],\n",
    "                            target[..., 1:3][obj])\n",
    "\n",
    "        # Calculate angle loss\n",
    "        angle_loss = self.mse(torch.sin(pred_angle[obj] - target[..., 5][obj]), torch.zeros_like(pred_angle[obj])) # fine with rad?\n",
    "        # Claculating class loss\n",
    "        class_loss = self.cross_entropy((pred[..., 6:][obj]),\n",
    "                                        target[..., 6][obj].long())\n",
    "\n",
    "        # print('~~~~~~~~~~~~~ FIND UNCAPPED ~~~~~~~~~~~~~')\n",
    "        # print('box_loss:', box_loss)\n",
    "        # print('angle_loss:', angle_loss)\n",
    "        # print('object_loss:', object_loss)\n",
    "        # print('no_object_loss:', no_object_loss)\n",
    "        # print('class_loss:', class_loss)\n",
    "        # print('reg_object_loss', reg_object_loss)\n",
    "\n",
    "        # Total loss\n",
    "        return (\n",
    "            box_loss\n",
    "            + box_loss_center\n",
    "            + angle_loss\n",
    "            + object_loss\n",
    "            + no_object_loss\n",
    "            + class_loss\n",
    "            # + reg_object_loss\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = YOLOLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(outputs[0], y0, scaled_anchors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obj_d",
   "language": "python",
   "name": "obj_d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
