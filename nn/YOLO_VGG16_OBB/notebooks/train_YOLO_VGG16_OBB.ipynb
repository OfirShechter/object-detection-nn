{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd object-detection-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_mode = True\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from nn.YOLO_VGG16_OBB.utils.constants import ANCHORS\n",
    "from nn.YOLO_VGG16_OBB.prepare_data.dota_dataset_memory import DotaDataset\n",
    "from nn.YOLO_VGG16_OBB.prepare_data.transforms import train_transform, test_transform\n",
    "from nn.YOLO_VGG16_OBB.utils.helpers import convert_cells_to_bboxes, load_checkpoint, nms, plot_image, save_checkpoint\n",
    "from nn.YOLO_VGG16_OBB.utils.constants import device, s, leanring_rate, save_model, checkpoint_file\n",
    "from nn.YOLO_VGG16_OBB.model.YOLO_VGG16_OBB import YOLO_VGG16_OBB\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from nn.YOLO_VGG16_OBB.model.loss import YOLOLoss\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as T\n",
    "\n",
    "if remote_mode:\n",
    "    model_path_base = f\"/home/dcor/niskhizov/Rar/object-detection-nn/nn/YOLO_VGG16_OBB/notebooks/vgg_f_obb_model\"\n",
    "else:\n",
    "    model_path_base = f\"nn/YOLO_VGG16_OBB/notebooks/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['plane','ship', 'storage-tank', 'baseball-diamond', 'tennis-court', 'basketball-court', 'ground-track-field', 'harbor', 'bridge', 'large-vehicle', 'small-vehicle', 'helicopter', 'roundabout', 'soccer-ball-field', 'swimming-pool']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model from YOLOv3 class \n",
    "load_model = True\n",
    "save_model = True\n",
    "model = YOLO_VGG16_OBB(num_classes=len(categories)).to(device) \n",
    "\n",
    "# Defining the optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr = leanring_rate) \n",
    "\n",
    "# Defining the loss function \n",
    "loss_fn = YOLOLoss() \n",
    "\n",
    "# Defining the scaler for mixed precision training \n",
    "scaler = torch.amp.GradScaler(device=device) \n",
    "# Loading the checkpoint \n",
    "if load_model: \n",
    "    load_checkpoint(model_path_base + f\"e2700_vgg16_{checkpoint_file}\", model, optimizer, leanring_rate, device) \n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='runs/YOLO_VGG16_OBB_v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DotaDataset( \n",
    "\tcategories=categories,\n",
    "\tgrid_sizes=[13, 26, 52], \n",
    "\tanchors=ANCHORS, \n",
    "\ttransform=train_transform \n",
    ") \n",
    "\n",
    "# Defining the train data loader \n",
    "train_loader = torch.utils.data.DataLoader( \n",
    "\tdataset=dataset, \n",
    "\tbatch_size=2, \n",
    "\tshuffle=True, \n",
    "\tnum_workers=2,\n",
    " \tprefetch_factor=10,\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = DotaDataset(\n",
    "    categories=categories,\n",
    "    grid_sizes=[13, 26, 52],\n",
    "    anchors=ANCHORS,\n",
    "    transform=test_transform,  # Use the same transform for validation\n",
    "    data_base_path = f\"nn/dotadataset/train\"\n",
    ")\n",
    "\n",
    "# Create the validation data loader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the anchors \n",
    "scaled_anchors = ( \n",
    "\ttorch.tensor(ANCHORS) *\n",
    "\ttorch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1,3,2) \n",
    ").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_iter = iter(val_loader)\n",
    "x, y = next(val_loader_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = YOLOLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100000000\n",
    "error_counter = 0\n",
    "save_model = True\n",
    "# Training the model \n",
    "for e in range(3142, epochs+1): \n",
    "\tprint(\"Epoch:\", e) \n",
    "    ################# dos\n",
    "    # Creating a progress bar \n",
    "\tprogress_bar = tqdm(train_loader, leave=True) \n",
    "\n",
    "\t# Initializing a list to store the losses \n",
    "\tlosses = [] \n",
    "\ttry:\n",
    "\t\t# Iterating over the training data \n",
    "\t\tfor batch_idx, (x, y) in enumerate(progress_bar):\n",
    "\t\t\t# try:\n",
    "\t\t\t\tprint(\"batch_idx:\", batch_idx)\n",
    "\t\t\t\tx = x.to(device) \n",
    "\t\t\t\ty0, y1, y2 = ( \n",
    "\t\t\t\t\ty[0].to(device), \n",
    "\t\t\t\t\ty[1].to(device), \n",
    "\t\t\t\t\ty[2].to(device), \n",
    "\t\t\t\t) \n",
    "\n",
    "\t\t\t\twith torch.amp.autocast(device_type=device): \n",
    "\t\t\t\t\t# Getting the model predictions \n",
    "\t\t\t\t\toutputs = model(x) \n",
    "\t\t\t\t\t# Calculating the loss at each scale \n",
    "\t\t\t\t\tloss = ( \n",
    "\t\t\t\t\t\tloss_fn(outputs[0], y0, scaled_anchors[0]) \n",
    "\t\t\t\t\t\t+ loss_fn(outputs[1], y1, scaled_anchors[1]) \n",
    "\t\t\t\t\t\t+ loss_fn(outputs[2], y2, scaled_anchors[2]) \n",
    "\t\t\t\t\t) \n",
    "\n",
    "\t\t\t\t# Add the loss to the list \n",
    "\t\t\t\tlosses.append(loss.item()) \n",
    "\n",
    "\t\t\t\t# Reset gradients \n",
    "\t\t\t\toptimizer.zero_grad() \n",
    "\n",
    "\t\t\t\t# Backpropagate the loss \n",
    "\t\t\t\tscaler.scale(loss).backward() \n",
    "\n",
    "\t\t\t\t# Optimization step \n",
    "\t\t\t\tscaler.step(optimizer) \n",
    "\n",
    "\t\t\t\t# Update the scaler for next iteration \n",
    "\t\t\t\tscaler.update() \n",
    "\n",
    "\t\t\t\t# update progress bar with loss \n",
    "\t\t\t\tmean_loss = sum(losses) / len(losses) \n",
    "\t\t\t\tprogress_bar.set_postfix(loss=mean_loss)\n",
    "\t\t\n",
    "\t\t\t\t# Log the loss to TensorBoard\n",
    "\t\t\t\twriter.add_scalar('Loss/train', mean_loss, e * len(train_loader) + batch_idx)\n",
    "\n",
    "\t\t\t\t# # Log images to TensorBoard every 100 batches\n",
    "\t\t\t\t# if batch_idx % 200 == 0:\n",
    "\t\t\t\t# \t# Saving the model \n",
    "\t\t\t\t# \tif save_model: \n",
    "\t\t\t\t# \t\tsave_checkpoint(model, optimizer, filename=model_path_base +f\"b{batch_idx}_vgg16_checkpoint.pth.tar\")\n",
    "\t\t\n",
    "\t\t\t\t# \tfor i in range(0, batch_idx-3, 200):\n",
    "\t\t\t\t# \t\t\tif os.path.exists(model_path_base + f\"b{i}_vgg16_checkpoint.pth.tar\"):\n",
    "\t\t\t\t# \t\t\t\tos.remove(model_path_base + f\"b{i}_vgg16_checkpoint.pth.tar\")\n",
    "\n",
    "\t\tif e % 100 == 0:\n",
    "\t\t\t# Saving the model \n",
    "\t\t\tif save_model: \n",
    "\t\t\t\tsave_checkpoint(model, optimizer, filename=model_path_base +f\"e{e}_vgg16_checkpoint.pth.tar\")\n",
    "\t\t\t\t# delete batch checkpoints\n",
    "\t\t\t\tfor i in range(0, batch_idx+1, 200):\n",
    "\t\t\t\t\tif os.path.exists(model_path_base + f\"b{i}_vgg16_checkpoint.pth.tar\"):\n",
    "\t\t\t\t\t\tos.remove(model_path_base + f\"b{i}_vgg16_checkpoint.pth.tar\")\n",
    "\t\t\t\t# for i in range(0,e-3):\n",
    "\t\t\t\t# \tif os.path.exists(model_path_base + f\"e{i}_vgg16_checkpoint.pth.tar\"):\n",
    "\t\t\t\t# \t\tos.remove(model_path_base + f\"e{i}_vgg16_checkpoint.pth.tar\")\n",
    "\t\t\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\n",
    "\t\t\t# Getting a sample image from the test data loader \n",
    "\t\t\ttry:\n",
    "\t\t\t\tx, y = next(val_loader_iter)\n",
    "\t\t\texcept StopIteration:\n",
    "\t\t\t\tval_loader_iter = iter(val_loader)\n",
    "\t\t\t\tx, y = next(val_loader_iter)\n",
    "\t\t\tx = x.to(device) \n",
    "\t\t\t\n",
    "\t\t\tprint(\"###################################### display and report image ######################################\")\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tscaled_anchors = ( \n",
    "\t\t\t\ttorch.tensor(ANCHORS) *\n",
    "\t\t\t\ttorch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1,3,2) \n",
    "\t\t\t\t).to(device) \n",
    "\t\t\t\toutput = model(x)\n",
    "\t\t\t\ty0, y1, y2 = ( \n",
    "\t\t\t\t\ty[0].to(device), \n",
    "\t\t\t\t\ty[1].to(device), \n",
    "\t\t\t\t\ty[2].to(device), \n",
    "\t\t\t\t) \n",
    "\n",
    "\t\t\t\twith torch.amp.autocast(device_type=device): \n",
    "\t\t\t\t\t# Getting the model predictions \n",
    "\t\t\t\t\toutputs = model(x) \n",
    "\t\t\t\t\t# Calculating the loss at each scale \n",
    "\t\t\t\t\tloss = ( \n",
    "\t\t\t\t\t\tloss_fn(outputs[0], y0, scaled_anchors[0]) \n",
    "\t\t\t\t\t\t+ loss_fn(outputs[1], y1, scaled_anchors[1]) \n",
    "\t\t\t\t\t\t+ loss_fn(outputs[2], y2, scaled_anchors[2]) \n",
    "\t\t\t\t\t) \n",
    "\t\t\n",
    "\t\t\t\t# Print predictions\n",
    "\t\t\t\twriter.add_scalar('Loss/val', loss.item(), e * len(train_loader) + batch_idx)\n",
    "\n",
    "\t\t\t\t# bboxes = [[] for _ in range(x.shape[0])]\n",
    "\t\t\t\t# for i in range(3):\n",
    "\t\t\t\t# \tbatch_size, A, S, _, _ = output[i].shape\n",
    "\t\t\t\t# \tanchor = scaled_anchors[i]\n",
    "\t\t\t\t# \tboxes_scale_i = convert_cells_to_bboxes(output[i], anchor, s=S, is_predictions=True)\n",
    "\t\t\t\t# \tfor idx, box in enumerate(boxes_scale_i):\n",
    "\t\t\t\t# \t\tbboxes[idx] += box\n",
    "\t\t\n",
    "\t\t\t\t# i = 0\n",
    "\t\t\t\t# print('bboxes[i] shape:', np.array(bboxes[i]).shape)\n",
    "\t\t\t\t# nms_boxes = nms(bboxes[i], iou_threshold=0.5, threshold=0.6)\n",
    "\t\t\t\t# img_with_boxes = plot_image(x[i].permute(1, 2, 0).detach().cpu(), nms_boxes, categories)\n",
    "\t\t\t\t# img_with_boxes = T.ToTensor()(img_with_boxes)\n",
    "\t\t\t\t# writer.add_image(f'Val/Image_{e}_{i}_{batch_idx}_before', img_with_boxes, e * len(train_loader) + batch_idx)\n",
    "\n",
    "\t\t\tmodel.train()\n",
    "\texcept Exception as error:\n",
    "\t\tprint(error)\n",
    "\t\terror_counter += 1\n",
    "\t\tif error_counter > 10:\n",
    "\t\t\traise error\n",
    "\n",
    "\n",
    "    #################\n",
    "\t# training_loop(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_boxes = nms(bboxes[i], iou_threshold=0.5, threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nn.YOLO_VGG16_OBB.utils.constants import device\n",
    "# Defining a function to calculate Intersection over Union (IoU)\n",
    "\n",
    "\n",
    "def iou(box1, box2, is_pred=True):\n",
    "    if is_pred:\n",
    "        # IoU score for prediction and label\n",
    "        # box1 (prediction) and box2 (label) are both in [x, y, width, height, angle] format\n",
    "        # Convert boxes to polygons\n",
    "        polys1 = []\n",
    "        polys2 = []\n",
    "\n",
    "        # angle (box1[..., 4]) is in radian than- convert to degree\n",
    "        angle1 = box1[..., 4] * (torch.pi / 2)\n",
    "        angle2 = box2[..., 4] * (torch.pi / 2)\n",
    "        angle1_degree = torch.rad2deg(angle1)\n",
    "        angle2_degree = torch.rad2deg(angle2)\n",
    "\n",
    "        for i in range(box1.shape[0]):\n",
    "            poly1 = cv2.boxPoints(((box1[i, 0].item(), box1[i, 1].item(\n",
    "            )), (box1[i, 2].item(), box1[i, 3].item()), angle1_degree[i].item()))\n",
    "            poly2 = cv2.boxPoints(((box2[i, 0].item(), box2[i, 1].item(\n",
    "            )), (box2[i, 2].item(), box2[i, 3].item()), angle2_degree[i].item()))\n",
    "            polys1.append(poly1)\n",
    "            polys2.append(poly2)\n",
    "\n",
    "        # Convert polygons to torch tensors\n",
    "        poly1 = np.array(polys1, dtype=np.float32)\n",
    "        poly2 = np.array(polys2, dtype=np.float32)\n",
    "        # Calculate intersection area\n",
    "        inter_area = polygon_intersection_area(poly1, poly2)\n",
    "        # Calculate union area\n",
    "        box1_area = box1[..., 2] * box1[..., 3]\n",
    "        box2_area = box2[..., 2] * box2[..., 3]\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "        # Calculate IoU score\n",
    "        epsilon = 1e-6\n",
    "        iou_score = inter_area / (union_area + epsilon)\n",
    "\n",
    "        # Return IoU score\n",
    "        return iou_score.unsqueeze(1)\n",
    "\n",
    "    else:\n",
    "        # IoU score based on width and height of bounding boxes\n",
    "\n",
    "        # Calculate intersection area\n",
    "        intersection_area = torch.min(\n",
    "            box1[..., 0], box2[..., 0]) * torch.min(box1[..., 1], box2[..., 1])\n",
    "\n",
    "        # Calculate union area\n",
    "        box1_area = box1[..., 0] * box1[..., 1]\n",
    "        box2_area = box2[..., 0] * box2[..., 1]        \n",
    "        union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "        # Calculate IoU score\n",
    "        iou_score = intersection_area / union_area\n",
    "\n",
    "        # Return IoU score\n",
    "        return iou_score.unsqueeze(1)\n",
    "\n",
    "\n",
    "\n",
    "def polygon_intersection_area(poly1_np, poly2_np):\n",
    "    # Ensure tensors are on CPU and convert to NumPy\n",
    "    # poly1_np = poly1.detach().cpu().numpy().astype(np.float32)\n",
    "    # poly2_np = poly2.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    inter_areas = []\n",
    "    for p1, p2 in zip(poly1_np, poly2_np):\n",
    "        inter_poly = cv2.intersectConvexConvex(p1, p2)\n",
    "        if inter_poly[0] > 0 and inter_poly[1] is not None:\n",
    "            inter_areas.append(cv2.contourArea(inter_poly[1]))\n",
    "        else:\n",
    "            inter_areas.append(0.0)\n",
    "    inter_areas = np.array(inter_areas, dtype=np.float32)\n",
    "    # Convert result back to a tensor\n",
    "    return torch.tensor(inter_areas, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "def nms(bboxes_orig, iou_threshold, threshold):\n",
    "    # Filter out bounding boxes with confidence below the threshold.\n",
    "    bboxes = [box for box in bboxes_orig if box[1] > threshold]\n",
    "\n",
    "    # Sort the bounding boxes by confidence in descending order.\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Initialize the list of bounding boxes after non-maximum suppression.\n",
    "    if (len(bboxes) > 0):\n",
    "        first_box = bboxes.pop(0)\n",
    "        bboxes_nms = [first_box]\n",
    "    else:\n",
    "        bboxes_nms = [max(bboxes_orig, key=lambda x: x[1])]\n",
    "\n",
    "    while len(bboxes) >= 0:\n",
    "        # Iterate over the remaining bounding boxes.\n",
    "        for box in bboxes:\n",
    "            # If the bounding boxes do not overlap or if the first bounding box has\n",
    "            # a higher confidence, then add the second bounding box to the list of\n",
    "            # bounding boxes after non-maximum suppression.\n",
    "            if box[0] != first_box[0] or iou(\n",
    "                    torch.tensor([first_box[2:]], device=device),\n",
    "                    torch.tensor([box[2:]], device=device),\n",
    "            ) < iou_threshold:\n",
    "                # Check if box is not in bboxes_nms\n",
    "                if box not in bboxes_nms:\n",
    "                    # Add box to bboxes_nms\n",
    "                    bboxes_nms.append(box)\n",
    "\n",
    "        # Get the first bounding box.\n",
    "        if len(bboxes) > 0:\n",
    "            first_box = bboxes.pop(0)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Return bounding boxes after non-maximum suppression.\n",
    "    # box concist: [class_pred, score, x, y, width, height, angle]\n",
    "    return bboxes_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "sigmoid(outputs[0][0][..., 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from nn.YOLO_VGG16_OBB.utils.helpers import iou\n",
    "\n",
    "# Defining YOLO loss class\n",
    "\n",
    "\n",
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.than = nn.Tanh()\n",
    "\n",
    "    def forward(self, pred, target, anchors):\n",
    "        # Identifying which cells in target have objects\n",
    "        # and which have no objects\n",
    "        obj = target[..., 0] == 1\n",
    "        no_obj = target[..., 0] == 0\n",
    "\n",
    "        # Calculating No object loss\n",
    "        no_object_loss = 10 * self.bce(\n",
    "            self.sigmoid(pred[..., 0:1][no_obj]), (target[..., 0:1][no_obj]),\n",
    "        )\n",
    "        \n",
    "        # reg_object_loss = 10 * self.bce(\n",
    "        #     self.sigmoid(pred[..., 0:1][obj]), (target[..., 0:1][obj]),\n",
    "        # )\n",
    "\n",
    "        # Reshaping anchors to match predictions\n",
    "        anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
    "        pred_angle = torch.tanh(pred[..., 5])\n",
    "        # Box prediction confidence\n",
    "        box_preds = torch.cat([self.sigmoid(pred[..., 1:3]),\n",
    "                               torch.exp(pred[..., 3:5]) *\n",
    "                               anchors, pred_angle.unsqueeze(-1)\n",
    "                               ], dim=-1)\n",
    "        # Calculating intersection over union for prediction and target\n",
    "        ious = iou(box_preds[obj], target[..., 1:6][obj]).detach()\n",
    "        # Calculating Object loss\n",
    "        object_loss = 10 * self.mse(self.sigmoid(pred[..., 0:1][obj]),\n",
    "                               ious * target[..., 0:1][obj]).clamp(0, 10)\n",
    "\n",
    "        # Predicted box coordinates\n",
    "        pred[..., 1:3] = self.sigmoid(pred[..., 1:3])\n",
    "        # Target box coordinates\n",
    "        target[..., 3:5] = torch.log(1e-6 + target[..., 3:5] / anchors)\n",
    "        # Calculating box coordinate loss\n",
    "\n",
    "        box_loss = self.mse(pred[..., 1:5][obj],\n",
    "                            target[..., 1:5][obj])\n",
    "\n",
    "        # Calculate angle loss\n",
    "        angle_loss = 10 * self.mse(pred_angle[obj],\n",
    "                              target[..., 5][obj])\n",
    "        # Claculating class loss\n",
    "        class_loss = self.cross_entropy((pred[..., 6:][obj]),\n",
    "                                        target[..., 6][obj].long())\n",
    "\n",
    "        # print('~~~~~~~~~~~~~ FIND UNCAPPED ~~~~~~~~~~~~~')\n",
    "        # print('box_loss:', box_loss)\n",
    "        # print('angle_loss:', angle_loss)\n",
    "        # print('object_loss:', object_loss)\n",
    "        # print('no_object_loss:', no_object_loss)\n",
    "        # print('class_loss:', class_loss)\n",
    "        # print('reg_object_loss', reg_object_loss)\n",
    "\n",
    "        # Total loss\n",
    "        return (\n",
    "            box_loss\n",
    "            + angle_loss\n",
    "            + object_loss\n",
    "            + no_object_loss\n",
    "            + class_loss\n",
    "            # + reg_object_loss\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obj_d",
   "language": "python",
   "name": "obj_d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
