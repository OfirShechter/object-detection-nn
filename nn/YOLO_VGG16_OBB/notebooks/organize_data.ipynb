{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd object-detection-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_mode = True\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from nn.YOLO_VGG16_OBB.utils.constants import ANCHORS\n",
    "from nn.YOLO_VGG16_OBB.prepare_data.dota_dataset_memory import DotaDataset\n",
    "from nn.YOLO_VGG16_OBB.prepare_data.transforms import train_transform, test_transform\n",
    "from nn.YOLO_VGG16_OBB.utils.helpers import convert_cells_to_bboxes, load_checkpoint, nms, plot_image, save_checkpoint\n",
    "from nn.YOLO_VGG16_OBB.utils.constants import device, s, leanring_rate, save_model, checkpoint_file\n",
    "from nn.YOLO_VGG16_OBB.model.YOLO_VGG16_OBB import YOLO_VGG16_OBB\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from nn.YOLO_VGG16_OBB.model.loss import YOLOLoss\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as T\n",
    "\n",
    "if remote_mode:\n",
    "    model_path_base = f\"/home/dcor/niskhizov/Rar/object-detection-nn/nn/YOLO_VGG16_OBB/notebooks/vgg_f_obb_model\"\n",
    "else:\n",
    "    model_path_base = f\"nn/YOLO_VGG16_OBB/notebooks/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['plane','ship', 'storage-tank', 'baseball-diamond', 'tennis-court', 'basketball-court', 'ground-track-field', 'harbor', 'bridge', 'large-vehicle', 'small-vehicle', 'helicopter', 'roundabout', 'soccer-ball-field', 'swimming-pool']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model from YOLOv3 class \n",
    "load_model = True\n",
    "save_model = False\n",
    "model = YOLO_VGG16_OBB(num_classes=len(categories)).to(device) \n",
    "\n",
    "# Defining the optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr = leanring_rate) \n",
    "\n",
    "# Defining the loss function \n",
    "loss_fn = YOLOLoss() \n",
    "\n",
    "# Defining the scaler for mixed precision training \n",
    "scaler = torch.amp.GradScaler(device=device) \n",
    "# Loading the checkpoint \n",
    "if load_model: \n",
    "    load_checkpoint(model_path_base + f\"e3300_vgg16_{checkpoint_file}\", model, optimizer, leanring_rate, device) \n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "# writer = SummaryWriter(log_dir='runs/YOLO_VGG16_OBB_v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DotaDataset( \n",
    "\tcategories=categories,\n",
    "\tgrid_sizes=[13, 26, 52], \n",
    "\tanchors=ANCHORS, \n",
    "\ttransform=train_transform \n",
    ") \n",
    "\n",
    "# Defining the train data loader \n",
    "train_loader = torch.utils.data.DataLoader( \n",
    "\tdataset=dataset, \n",
    "\tbatch_size=8, \n",
    "\tshuffle=True, \n",
    "\tnum_workers=2,\n",
    " \tprefetch_factor=10,\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = DotaDataset(\n",
    "    categories=categories,\n",
    "    grid_sizes=[13, 26, 52],\n",
    "    anchors=ANCHORS,\n",
    "    transform=test_transform,  # Use the same transform for validation\n",
    "    data_base_path = f\"nn/dotadataset/train\"\n",
    ")\n",
    "\n",
    "# Create the validation data loader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the anchors \n",
    "scaled_anchors = ( \n",
    "\ttorch.tensor(ANCHORS) *\n",
    "\ttorch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1,3,2) \n",
    ").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Getting a sample image from the test data loader \n",
    "try:\n",
    "\tx, y = next(val_loader_iter)\n",
    "except StopIteration:\n",
    "\tval_loader_iter = iter(val_loader)\n",
    "\tx, y = next(val_loader_iter)\n",
    "x = x.to(device) \n",
    "\n",
    "print(\"###################################### display and report image ######################################\")\n",
    "with torch.no_grad():\n",
    "\tscaled_anchors = ( \n",
    "\ttorch.tensor(ANCHORS) *\n",
    "\ttorch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1,3,2) \n",
    "\t).to(device) \n",
    "\toutput = model(x)\n",
    "\ty0, y1, y2 = ( \n",
    "\t\ty[0].to(device), \n",
    "\t\ty[1].to(device), \n",
    "\t\ty[2].to(device), \n",
    "\t) \n",
    "\n",
    "\twith torch.amp.autocast(device_type=device): \n",
    "\t\t# Getting the model predictions \n",
    "\t\toutputs = model(x) \n",
    "\t\t# Calculating the loss at each scale \n",
    "\t\tloss = ( \n",
    "\t\t\tloss_fn(outputs[0], y0, scaled_anchors[0]) \n",
    "\t\t\t+ loss_fn(outputs[1], y1, scaled_anchors[1]) \n",
    "\t\t\t+ loss_fn(outputs[2], y2, scaled_anchors[2]) \n",
    "\t\t) \n",
    "\n",
    "\t# TEMP- print target boxes\n",
    "\tbboxes = [[] for _ in range(x.shape[0])]\n",
    "\tfor i in range(3):\n",
    "\t\tbatch_size, A, S, _, _ = y[i].shape\n",
    "\t\tanchor = scaled_anchors[i]\n",
    "\t\tboxes_scale_i = convert_cells_to_bboxes(output[i], anchor, s=S, is_predictions=True)\n",
    "\t\tfor idx, box in enumerate(boxes_scale_i):\n",
    "\t\t\tbboxes[idx] += box\n",
    "\n",
    "\ti = 0\n",
    "\tprint('bboxes[i] shape:', np.array(bboxes[i]).shape)\n",
    "\tnms_boxes = nms(bboxes[i], iou_threshold=0.6, threshold=0.8)\n",
    "\timg_with_boxes = plot_image(x[i].permute(1, 2, 0).detach().cpu(), nms_boxes, categories)\n",
    "\timg_with_boxes = T.ToTensor()(img_with_boxes)\n",
    "\n",
    "\t# # Print predictions\n",
    "\t# writer.add_scalar('Loss/val', loss.item(), e * len(train_loader) + batch_idx)\n",
    "\n",
    "\t# bboxes = [[] for _ in range(x.shape[0])]\n",
    "\t# for i in range(3):\n",
    "\t# \tbatch_size, A, S, _, _ = output[i].shape\n",
    "\t# \tanchor = scaled_anchors[i]\n",
    "\t# \tboxes_scale_i = convert_cells_to_bboxes(output[i], anchor, s=S, is_predictions=True)\n",
    "\t# \tfor idx, box in enumerate(boxes_scale_i):\n",
    "\t# \t\tbboxes[idx] += box\n",
    "\n",
    "\t# i = 0\n",
    "\t# print('bboxes[i] shape:', np.array(bboxes[i]).shape)\n",
    "\t# nms_boxes = nms(bboxes[i], iou_threshold=0.5, threshold=0.6)\n",
    "\t# img_with_boxes = plot_image(x[i].permute(1, 2, 0).detach().cpu(), nms_boxes, categories)\n",
    "\t# img_with_boxes = T.ToTensor()(img_with_boxes)\n",
    "\t# writer.add_image(f'Val/Image_{e}_{i}_{batch_idx}_before', img_with_boxes, e * len(train_loader) + batch_idx)\n",
    "\n",
    "# model.train()\n",
    "# except Exception as error:\n",
    "# \tprint(error)\n",
    "# \terror_counter += 1\n",
    "# \tif error_counter > 10:\n",
    "# \t\traise error\n",
    "\n",
    "\n",
    "#################\n",
    "# training_loop(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nn.YOLO_VGG16_OBB.utils.constants import device\n",
    "# Defining a function to calculate Intersection over Union (IoU)\n",
    "\n",
    "\n",
    "def iou(box1, box2, is_pred=True):\n",
    "    if is_pred:\n",
    "        # IoU score for prediction and label\n",
    "        # box1 (prediction) and box2 (label) are both in [x, y, width, height, angle] format\n",
    "        # Convert boxes to polygons\n",
    "        polys1 = []\n",
    "        polys2 = []\n",
    "\n",
    "        # angle (box1[..., 4]) is in radian than- convert to degree\n",
    "        angle1 = box1[..., 4] * (torch.pi / 2)\n",
    "        angle2 = box2[..., 4] * (torch.pi / 2)\n",
    "        angle1_degree = torch.rad2deg(angle1)\n",
    "        angle2_degree = torch.rad2deg(angle2)\n",
    "\n",
    "        for i in range(box1.shape[0]):\n",
    "            poly1 = cv2.boxPoints(((box1[i, 0].item(), box1[i, 1].item(\n",
    "            )), (box1[i, 2].item(), box1[i, 3].item()), angle1_degree[i].item()))\n",
    "            poly2 = cv2.boxPoints(((box2[i, 0].item(), box2[i, 1].item(\n",
    "            )), (box2[i, 2].item(), box2[i, 3].item()), angle2_degree[i].item()))\n",
    "            polys1.append(poly1)\n",
    "            polys2.append(poly2)\n",
    "\n",
    "        # Convert polygons to torch tensors\n",
    "        poly1 = np.array(polys1, dtype=np.float32)\n",
    "        poly2 = np.array(polys2, dtype=np.float32)\n",
    "        # Calculate intersection area\n",
    "        inter_area = polygon_intersection_area(poly1, poly2)\n",
    "        # Calculate union area\n",
    "        box1_area = box1[..., 2] * box1[..., 3]\n",
    "        box2_area = box2[..., 2] * box2[..., 3]\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "        # Calculate IoU score\n",
    "        epsilon = 1e-6\n",
    "        iou_score = inter_area / (union_area + epsilon)\n",
    "\n",
    "        # Return IoU score\n",
    "        return iou_score.unsqueeze(1)\n",
    "\n",
    "    else:\n",
    "        # IoU score based on width and height of bounding boxes\n",
    "\n",
    "        # Calculate intersection area\n",
    "        intersection_area = torch.min(\n",
    "            box1[..., 0], box2[..., 0]) * torch.min(box1[..., 1], box2[..., 1])\n",
    "\n",
    "        # Calculate union area\n",
    "        box1_area = box1[..., 0] * box1[..., 1]\n",
    "        box2_area = box2[..., 0] * box2[..., 1]\n",
    "        union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "        # Calculate IoU score\n",
    "        iou_score = intersection_area / union_area\n",
    "\n",
    "        # Return IoU score\n",
    "        return iou_score.unsqueeze(1)\n",
    "\n",
    "\n",
    "\n",
    "def polygon_intersection_area(poly1_np, poly2_np):\n",
    "    # Ensure tensors are on CPU and convert to NumPy\n",
    "    # poly1_np = poly1.detach().cpu().numpy().astype(np.float32)\n",
    "    # poly2_np = poly2.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    inter_areas = []\n",
    "    for p1, p2 in zip(poly1_np, poly2_np):\n",
    "        inter_poly = cv2.intersectConvexConvex(p1, p2)\n",
    "        if inter_poly[0] > 0 and inter_poly[1] is not None:\n",
    "            inter_areas.append(cv2.contourArea(inter_poly[1]))\n",
    "        else:\n",
    "            inter_areas.append(0.0)\n",
    "    inter_areas = np.array(inter_areas, dtype=np.float32)\n",
    "    # Convert result back to a tensor\n",
    "    return torch.tensor(inter_areas, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "def nms(bboxes_orig, iou_threshold, threshold):\n",
    "    # Filter out bounding boxes with confidence below the threshold.\n",
    "    bboxes = [box for box in bboxes_orig if box[1] > threshold]\n",
    "\n",
    "    # Sort the bounding boxes by confidence in descending order.\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Initialize the list of bounding boxes after non-maximum suppression.\n",
    "    if (len(bboxes) > 0):\n",
    "        first_box = bboxes.pop(0)\n",
    "        bboxes_nms = [first_box]\n",
    "    else:\n",
    "        bboxes_nms = [max(bboxes_orig, key=lambda x: x[1])]\n",
    "\n",
    "    while len(bboxes) >= 0:\n",
    "        # Iterate over the remaining bounding boxes.\n",
    "        for box in bboxes:\n",
    "            # If the bounding boxes do not overlap or if the first bounding box has\n",
    "            # a higher confidence, then add the second bounding box to the list of\n",
    "            # bounding boxes after non-maximum suppression.\n",
    "            if box[0] != first_box[0] or iou(\n",
    "                    torch.tensor([first_box[2:]], device=device),\n",
    "                    torch.tensor([box[2:]], device=device),\n",
    "            ) < iou_threshold:\n",
    "                # Check if box is not in bboxes_nms\n",
    "                if box not in bboxes_nms:\n",
    "                    # Add box to bboxes_nms\n",
    "                    bboxes_nms.append(box)\n",
    "\n",
    "        # Get the first bounding box.\n",
    "        if len(bboxes) > 0:\n",
    "            first_box = bboxes.pop(0)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Return bounding boxes after non-maximum suppression.\n",
    "    # box concist: [class_pred, score, x, y, width, height, angle]\n",
    "    return bboxes_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for box in bboxes[0]: \n",
    "    print(box[1])\n",
    "    if box[1] > 0.03: \n",
    "        print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nn.YOLO_VGG16_OBB.utils.helpers import iou\n",
    "\n",
    "def plot_image(image, boxes, labels, display=True):\n",
    "    # Getting the color map from matplotlib\n",
    "    colour_map = plt.get_cmap(\"tab20b\")\n",
    "\n",
    "    # Convert image to NumPy array (if not already)\n",
    "    img = np.array(image)\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    # Copy the image to avoid modifying the original\n",
    "    img_drawn = img.copy()\n",
    "\n",
    "    # Plot bounding boxes and labels\n",
    "    for box in boxes:\n",
    "        class_pred = int(box[0])\n",
    "        cx, cy, bw, bh, angle = box[2:]\n",
    "\n",
    "        # Convert to absolute coordinates\n",
    "        cx, cy, bw, bh = cx * w, cy * h, bw * w, bh * h\n",
    "\n",
    "        # Get color\n",
    "        color = tuple(int(c * 255) for c in colour_map(class_pred + 100)[:3])\n",
    "        if angle > 10:\n",
    "            print(angle)\n",
    "        # Get rotated rectangle\n",
    "        rect = ((cx, cy), (bw, bh), angle)  # OpenCV expects angle in degrees\n",
    "        box_points = cv2.boxPoints(rect)  # Get corner points\n",
    "        box_points = np.int32(box_points)  # Convert to integer\n",
    "\n",
    "        # Draw the rotated rectangle\n",
    "        cv2.polylines(img_drawn, [box_points], isClosed=True, color=color, thickness=1)\n",
    "\n",
    "        # Put label text near the rectangle\n",
    "        # label = labels[class_pred]\n",
    "        # (text_width, text_height), baseline = cv2.getTextSize(\n",
    "        #     label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "        # text_x, text_y = int(cx - text_width / 2), int(cy - bh / 2 - 10)\n",
    "        # cv2.rectangle(img_drawn, (text_x, text_y - text_height - 4),\n",
    "        #               (text_x + text_width, text_y), color, -1)\n",
    "        # cv2.putText(img_drawn, label, (text_x, text_y),\n",
    "        #             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    if display:\n",
    "        # Display the image\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cv2.cvtColor(img_drawn, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return img_drawn  # Return the modified image with drawn bounding boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(bboxes_orig, iou_threshold, threshold):\n",
    "    # Filter out bounding boxes with confidence below the threshold.\n",
    "    bboxes = [box for box in bboxes_orig if box[1] > threshold]\n",
    "\n",
    "    # Sort the bounding boxes by confidence in descending order.\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Initialize the list of bounding boxes after non-maximum suppression.\n",
    "    if (len(bboxes) > 0):\n",
    "        first_box = bboxes.pop(0)\n",
    "        bboxes_nms = [first_box]\n",
    "    else:\n",
    "        bboxes_nms = [max(bboxes_orig, key=lambda x: x[1])]\n",
    "\n",
    "    while len(bboxes) >= 0:\n",
    "        # Iterate over the remaining bounding boxes.\n",
    "        for box in bboxes:\n",
    "            # If the bounding boxes do not overlap or if the first bounding box has\n",
    "            # a higher confidence, then add the second bounding box to the list of\n",
    "            # bounding boxes after non-maximum suppression.\n",
    "            if box[0] != first_box[0] or iou(\n",
    "                    torch.tensor([first_box[2:]]),\n",
    "                    torch.tensor([box[2:]]),\n",
    "            ) < iou_threshold:\n",
    "                # Check if box is not in bboxes_nms\n",
    "                if box not in bboxes_nms:\n",
    "                    # Add box to bboxes_nms\n",
    "                    bboxes_nms.append(box)\n",
    "\n",
    "        # Get the first bounding box.\n",
    "        if len(bboxes) > 0:\n",
    "            first_box = bboxes.pop(0)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Return bounding boxes after non-maximum suppression.\n",
    "    return bboxes_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = target[..., 0] == 1\n",
    "no_obj = target[..., 0] == 0\n",
    "\n",
    "# Calculating No object loss \n",
    "no_object_loss = 4 * bce( \n",
    "    (pred[..., 0:1][no_obj]), (target[..., 0:1][no_obj]), \n",
    ") \n",
    "\n",
    "\n",
    "# Reshaping anchors to match predictions \n",
    "anchors = anchors.reshape(1, 3, 1, 1, 2) \n",
    "# Box prediction confidence \n",
    "box_preds = torch.cat([sigmoid(pred[..., 1:3]), \n",
    "                    torch.exp(pred[..., 3:5]) * anchors, pred[..., 5].unsqueeze(-1)\n",
    "                    ],dim=-1) \n",
    "# Calculating intersection over union for prediction and target \n",
    "ious = iou(box_preds[obj], target[..., 1:6][obj]).detach() \n",
    "ious = ious.unsqueeze(1)\n",
    "object_loss = mse(sigmoid(pred[..., 0:1][obj]), \n",
    "                    ious * target[..., 0:1][obj]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_preds[obj].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_bboxes = torch.cat(\n",
    "    (best_class, scores, x, y, width_height, angle.unsqueeze(0)), dim=-1\n",
    ").reshape(batch_size, num_anchors * s * s, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor.shape\n",
    "anchors = anchor.reshape(1, len(anchor), 1, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_predictions[..., 2:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_predictions = output[i][..., 1:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(box_predictions[..., 2:]) * anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(output[i][..., 2:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd nn/YOLO_VGG16/prepare_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f'../../cocodataset/images/train2017/000000111341.jpg'\n",
    "img = cv2.imread(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nn.YOLO_VGG16_OBB.utils.helpers import iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(bboxes[i]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_orig, iou_threshold, threshold = bboxes[i], 0.5, 0.6\n",
    "# Filter out bounding boxes with confidence below the threshold.\n",
    "bboxes = [box for box in bboxes_orig if box[1] > threshold]\n",
    "\n",
    "# Sort the bounding boxes by confidence in descending order.\n",
    "bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Initialize the list of bounding boxes after non-maximum suppression.\n",
    "if (len(bboxes) > 0):\n",
    "    first_box = bboxes.pop(0)\n",
    "    bboxes_nms = [first_box]\n",
    "else:\n",
    "    bboxes_nms = [max(bboxes_orig, key=lambda x: x[1])]\n",
    "\n",
    "while len(bboxes) >= 0:\n",
    "    # Iterate over the remaining bounding boxes.\n",
    "    for box in bboxes:\n",
    "        # If the bounding boxes do not overlap or if the first bounding box has\n",
    "        # a higher confidence, then add the second bounding box to the list of\n",
    "        # bounding boxes after non-maximum suppression.\n",
    "        if box[0] != first_box[0] or iou(\n",
    "                torch.tensor(first_box[1:]),\n",
    "                torch.tensor(box[1:]),\n",
    "        ) < iou_threshold:\n",
    "            print('**************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obj_d",
   "language": "python",
   "name": "obj_d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
