{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd object-detection-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_mode = True\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from nn.YOLO_VGG16_OBB.utils.constants import ANCHORS\n",
    "from nn.YOLO_VGG16_OBB.prepare_data.dota_dataset_memory import DotaDataset\n",
    "from nn.YOLO_VGG16_OBB.prepare_data.transforms import train_transform, test_transform\n",
    "from nn.YOLO_VGG16_OBB.utils.helpers import convert_cells_to_bboxes, load_checkpoint, nms, plot_image, save_checkpoint\n",
    "from nn.YOLO_VGG16_OBB.utils.constants import device, s, leanring_rate, save_model, checkpoint_file\n",
    "from nn.YOLO_VGG16_OBB.model.YOLO_VGG16_OBB import YOLO_VGG16_OBB\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from nn.YOLO_VGG16_OBB.model.loss import YOLOLoss\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as T\n",
    "\n",
    "if remote_mode:\n",
    "    model_path_base = f\"/home/dcor/niskhizov/Rar/object-detection-nn/nn/YOLO_VGG16_OBB/notebooks/vgg_f_obb_model\"\n",
    "else:\n",
    "    model_path_base = f\"nn/YOLO_VGG16_OBB/notebooks/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['plane','ship', 'storage-tank', 'baseball-diamond', 'tennis-court', 'basketball-court', 'ground-track-field', 'harbor', 'bridge', 'large-vehicle', 'small-vehicle', 'helicopter', 'roundabout', 'soccer-ball-field', 'swimming-pool']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model from YOLOv3 class \n",
    "load_model = False\n",
    "save_model = False\n",
    "model = YOLO_VGG16_OBB(num_classes=len(categories)).to(device) \n",
    "\n",
    "# Defining the optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr = leanring_rate) \n",
    "\n",
    "# Defining the loss function \n",
    "loss_fn = YOLOLoss() \n",
    "\n",
    "# Defining the scaler for mixed precision training \n",
    "scaler = torch.amp.GradScaler(device=device) \n",
    "# Loading the checkpoint \n",
    "if load_model: \n",
    "    load_checkpoint(model_path_base + f\"e108_b0_vgg16_{checkpoint_file}\", model, optimizer, leanring_rate, device) \n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir='runs/YOLO_VGG16_OBB_v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from nn.YOLO_VGG16_OBB.utils.helpers import iou\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# # Create a dataset class to load the images and labels from the folder\n",
    "\n",
    "\n",
    "# class DotaDataset(Dataset):\n",
    "#     def __init__(\n",
    "#             self, categories, anchors, transform=None, data_base_path=f\"nn/dotadataset/train\",\n",
    "#             image_size=416, grid_sizes=[13, 26, 52]\n",
    "#     ):\n",
    "#         self.images_path = f\"{data_base_path}/images\"\n",
    "#         self.labels_path = f\"{data_base_path}/labelTxt-v1.0\"\n",
    "#         self.img_ids = [os.path.splitext(f)[0] for f in os.listdir(\n",
    "#             self.images_path) if f.endswith('.png')]\n",
    "#         self.cat_ids_map = {category: i for i,\n",
    "#                             category in enumerate(categories)}\n",
    "#         self.img_ids = self.img_ids[:10]\n",
    "#         # Image size\n",
    "#         self.image_size = image_size\n",
    "#         # # Transformations\n",
    "#         self.transform = transform(\n",
    "#             image_size) if transform is not None else None\n",
    "#         # self.transform = None\n",
    "#         # Grid sizes for each scale\n",
    "#         self.grid_sizes = grid_sizes\n",
    "#         # Anchor boxes\n",
    "#         self.anchors = torch.tensor(\n",
    "#             anchors[0] + anchors[1] + anchors[2])\n",
    "#         # Number of anchor boxes\n",
    "#         self.num_anchors = self.anchors.shape[0]\n",
    "#         # Number of anchor boxes per scale\n",
    "#         self.num_anchors_per_scale = self.num_anchors // 3\n",
    "#         # Number of classes\n",
    "#         self.num_classes = len(categories)\n",
    "#         # Ignore IoU threshold\n",
    "#         self.ignore_iou_thresh = 0.5\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_ids)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         error_counter = 0\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 return self.getitem_helper(idx)\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#                 # choose random different idx\n",
    "#                 idx = np.random.randint(0, len(self.img_ids))\n",
    "#                 error_counter += 1\n",
    "#                 if error_counter > 10:\n",
    "#                     print(\"Too many errors\")\n",
    "#                     raise Exception(\"Too many errors\")\n",
    "\n",
    "#     def getitem_helper(self, idx):\n",
    "#         img_id = self.img_ids[idx]\n",
    "#         img_path = os.path.join(self.images_path, f\"{img_id}.png\")\n",
    "\n",
    "#         # Load image from memory\n",
    "#         img = cv2.imread(img_path)\n",
    "#         if img is None:\n",
    "#             raise Exception(f\"Failed to load image from {img_path}\")\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         img = np.array(Image.fromarray(img), dtype=np.float32)\n",
    "#         img_size_x = img.shape[1]\n",
    "#         img_size_y = img.shape[0]\n",
    "#         # Load labels\n",
    "#         label_path = os.path.join(self.labels_path, f\"{img_id}.txt\")\n",
    "#         if not os.path.exists(label_path):\n",
    "#             raise Exception(f\"Labels file not found: {label_path}\")\n",
    "\n",
    "#         bboxes = []\n",
    "#         angles = []\n",
    "#         with open(label_path, 'r') as f:\n",
    "#             lines = f.readlines()\n",
    "#             for line in lines:\n",
    "#                 parts = line.strip().split()\n",
    "#                 if len(parts) < 10:\n",
    "#                     continue  # Skip invalid lines\n",
    "#                 x1, y1, x2, y2, x3, y3, x4, y4 = map(float, parts[:8])\n",
    "#                 category = parts[8]\n",
    "#                 if category not in self.cat_ids_map:\n",
    "#                     raise Exception(f\"Unknown category: {category}\")\n",
    "#                 class_label = self.cat_ids_map[category]\n",
    "\n",
    "#                 # Convert OBB to (cx, cy, w, h, angle)\n",
    "#                 poly = np.array([[x1, y1], [x2, y2], [x3, y3], [\n",
    "#                                 x4, y4]], dtype=np.float32).reshape((-1, 1, 2))\n",
    "#                 rect = cv2.minAreaRect(poly)\n",
    "#                 (cx, cy), (w, h), angle = rect\n",
    "#                 n_cx, n_cy, n_w, n_h = cx / img_size_x, cy / img_size_y, w / img_size_x, h / img_size_y\n",
    "#                 if (n_cx < 0 or n_cy < 0 or n_w < 0 or n_h < 0) or (n_cx > 1 or n_cy > 1 or n_w > 1 or n_h > 1):\n",
    "#                     print('origin:', [x1, y1], [x2, y2], [x3, y3], [x4, y4])\n",
    "#                     print('poly', poly)\n",
    "#                     print('rect:', rect)\n",
    "#                     print('cx:', cx, 'cy:', cy, 'w:',\n",
    "#                           w, 'h:', h, 'angle:', angle)\n",
    "#                 # else:\n",
    "#                 #     print('No lower then 0:', n_cx, n_cy, n_w, n_h, angle)\n",
    "#                 bboxes.append([n_cx, n_cy, n_w, n_h, class_label])\n",
    "#                 angles.append(angle)\n",
    "#         if self.transform is not None:\n",
    "#             augs = self.transform(\n",
    "#                 image=img, bboxes=bboxes)\n",
    "#             img = augs[\"image\"]\n",
    "#             bboxes = [[cx, cy, w, h, angle, class_label] for (\n",
    "#                 cx, cy, w, h, class_label), angle in zip(augs[\"bboxes\"], angles)]\n",
    "#         else:\n",
    "#             bboxes = [[cx, cy, w, h, angle, class_label]\n",
    "#                       for (cx, cy, w, h, class_label), angle in zip(bboxes, angles)]\n",
    "#         # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
    "#         # target : [probabilities, x, y, width, height, angle, class_label]\n",
    "#         targets = [torch.zeros((self.num_anchors_per_scale, s, s, 7))\n",
    "#                    for s in self.grid_sizes]\n",
    "\n",
    "#         # Identify anchor box and cell for each bounding box\n",
    "#         for box in bboxes:\n",
    "#             # Calculate iou of bounding box with anchor boxes\n",
    "#             iou_anchors = iou(torch.tensor(box[2:4]),\n",
    "#                               self.anchors,\n",
    "#                               is_pred=False)\n",
    "#             # Selecting the best anchor box\n",
    "#             anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "#             x, y, width, height, angle, class_label = box\n",
    "\n",
    "#             if (x < 0 or y < 0 or width < 0 or height < 0) or (x > 1 or y > 1 or width > 1 or height > 1):\n",
    "#                 print('x:', x, 'y:', y, 'w:',\n",
    "#                         width, 'height:', height, 'angle:', angle)\n",
    "              \n",
    "#             if angle > 10 or angle < -10:\n",
    "#                 print('angle:', angle)\n",
    "#                 print('cordinates:', x, y, width, height)\n",
    "#                 print('img_id:', img_id)      \n",
    "#             # At each scale, assigning the bounding box to the\n",
    "#             # best matching anchor box\n",
    "#             has_anchor = [False] * 3\n",
    "\n",
    "#             for anchor_idx in anchor_indices:\n",
    "#                 scale_idx = anchor_idx // self.num_anchors_per_scale\n",
    "#                 anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "\n",
    "#                 # Identifying the grid size for the scale\n",
    "#                 s = self.grid_sizes[scale_idx]\n",
    "\n",
    "#                 # Identifying the cell to which the bounding box belongs\n",
    "#                 i, j = int(s * y), int(s * x)\n",
    "#                 anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "\n",
    "#                 # Check if the anchor box is already assigned\n",
    "#                 if not anchor_taken and not has_anchor[scale_idx]:\n",
    "\n",
    "#                     # Set the probability to 1\n",
    "#                     targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "\n",
    "#                     # Calculating the center of the bounding box relative\n",
    "#                     # to the cell\n",
    "#                     x_cell, y_cell = s * x - j, s * y - i\n",
    "\n",
    "#                     # Calculating the width and height of the bounding box\n",
    "#                     # relative to the cell\n",
    "#                     width_cell, height_cell = (width * s, height * s)\n",
    "\n",
    "#                     # Idnetify the box coordinates\n",
    "#                     box_coordinates = torch.tensor(\n",
    "#                         [x_cell, y_cell, width_cell,\n",
    "#                          height_cell, angle]\n",
    "#                     )\n",
    "\n",
    "#                     # Assigning the box coordinates to the target\n",
    "#                     targets[scale_idx][anchor_on_scale,\n",
    "#                                        i, j, 1:6] = box_coordinates\n",
    "\n",
    "#                     # Assigning the class label to the target\n",
    "#                     targets[scale_idx][anchor_on_scale,\n",
    "#                                        i, j, 6] = int(class_label)\n",
    "\n",
    "#                     # Set the anchor box as assigned for the scale\n",
    "#                     has_anchor[scale_idx] = True\n",
    "\n",
    "#                     # If the anchor box is already assigned, check if the\n",
    "#                     # IoU is greater than the threshold\n",
    "#                 elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "#                     # Set the probability to -1 to ignore the anchor box\n",
    "#                     targets[scale_idx][anchor_on_scale, i, j, 0] = -1\n",
    "\n",
    "#                 # Return the image and the target\n",
    "#         return img, tuple(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DotaDataset( \n",
    "\tcategories=categories,\n",
    "\tgrid_sizes=[13, 26, 52], \n",
    "\tanchors=ANCHORS, \n",
    "\ttransform=train_transform \n",
    ") \n",
    "\n",
    "# Defining the train data loader \n",
    "train_loader = torch.utils.data.DataLoader( \n",
    "\tdataset=dataset, \n",
    "\tbatch_size=8, \n",
    "\tshuffle=True, \n",
    "\tnum_workers=2,\n",
    " \tprefetch_factor=10,\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = DotaDataset(\n",
    "    categories=categories,\n",
    "    grid_sizes=[13, 26, 52],\n",
    "    anchors=ANCHORS,\n",
    "    transform=test_transform,  # Use the same transform for validation\n",
    "    data_base_path = f\"nn/dotadataset/train\"\n",
    ")\n",
    "\n",
    "# Create the validation data loader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the anchors \n",
    "scaled_anchors = ( \n",
    "\ttorch.tensor(ANCHORS) *\n",
    "\ttorch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1,3,2) \n",
    ").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from nn.YOLO_VGG16_OBB.utils.helpers import iou\n",
    "\n",
    "# def plot_image(image, boxes, labels, display=True):\n",
    "#     # Getting the color map from matplotlib\n",
    "#     colour_map = plt.get_cmap(\"tab20b\")\n",
    "\n",
    "#     # Convert image to NumPy array (if not already)\n",
    "#     img = np.array(image)\n",
    "#     h, w, _ = img.shape\n",
    "\n",
    "#     # Copy the image to avoid modifying the original\n",
    "#     img_drawn = img.copy()\n",
    "\n",
    "#     # Plot bounding boxes and labels\n",
    "#     for box in boxes:\n",
    "#         class_pred = int(box[0])\n",
    "#         cx, cy, bw, bh, angle = box[2:]\n",
    "\n",
    "#         # Convert to absolute coordinates\n",
    "#         cx, cy, bw, bh = cx * w, cy * h, bw * w, bh * h\n",
    "\n",
    "#         # Get color\n",
    "#         color = colour_map(class_pred)\n",
    "#         # Get rotated rectangle\n",
    "#         rect = ((cx, cy), (bw, bh), angle)  # OpenCV expects angle in degrees\n",
    "#         box_points = cv2.boxPoints(rect)  # Get corner points\n",
    "#         box_points = np.int32(box_points)  # Convert to integer\n",
    "\n",
    "#         # Draw the rotated rectangle\n",
    "#         cv2.polylines(img_drawn, [box_points], isClosed=True, color=color, thickness=2)\n",
    "\n",
    "#         # Put label text near the rectangle\n",
    "#         # label = labels[class_pred]\n",
    "#         # (text_width, text_height), baseline = cv2.getTextSize(\n",
    "#         #     label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "#         # text_x, text_y = int(cx - text_width / 2), int(cy - bh / 2 - 10)\n",
    "#         # cv2.rectangle(img_drawn, (text_x, text_y - text_height - 4),\n",
    "#         #               (text_x + text_width, text_y), color, -1)\n",
    "#         # cv2.putText(img_drawn, label, (text_x, text_y),\n",
    "#         #             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "#     if display:\n",
    "#         # Display the image\n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         plt.imshow(cv2.cvtColor(img_drawn, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.show()\n",
    "\n",
    "#     return img_drawn  # Return the modified image with drawn bounding boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_cells_to_bboxes(predictions, anchors, s, is_predictions=True):\n",
    "#     # Batch size used on predictions\n",
    "#     batch_size = predictions.shape[0]\n",
    "#     # Number of anchors\n",
    "#     num_anchors = len(anchors)\n",
    "#     # List of all the predictions\n",
    "#     box_predictions = predictions[..., 1:5]\n",
    "\n",
    "#     # If the input is predictions then we will pass the x and y coordinate\n",
    "#     # through sigmoid function and width and height to exponent function and\n",
    "#     # calculate the score and best class.\n",
    "#     if is_predictions:\n",
    "#         anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
    "#         box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
    "#         box_predictions[..., 2:] = torch.exp(\n",
    "#             box_predictions[..., 2:]) * anchors\n",
    "#         scores = torch.sigmoid(predictions[..., 0:1])\n",
    "#         best_class = torch.argmax(predictions[..., 6:], dim=-1).unsqueeze(-1)\n",
    "\n",
    "#     # Else we will just calculate scores and best class.\n",
    "#     else:\n",
    "#         scores = predictions[..., 0:1]\n",
    "#         best_class = predictions[..., 6:7]\n",
    "\n",
    "#     # Calculate cell indices\n",
    "#     cell_indices = (\n",
    "#         torch.arange(s)\n",
    "#         .repeat(predictions.shape[0], 3, s, 1)\n",
    "#         .unsqueeze(-1)\n",
    "#         .to(predictions.device)\n",
    "#     )\n",
    "\n",
    "#     # Calculate x, y, width and height with proper scaling\n",
    "#     x = 1 / s * (box_predictions[..., 0:1] + cell_indices)\n",
    "#     y = 1 / s * (box_predictions[..., 1:2] +\n",
    "#                  cell_indices.permute(0, 1, 3, 2, 4))\n",
    "#     width_height = 1 / s * box_predictions[..., 2:4]\n",
    "#     angle = predictions[..., 5:6]\n",
    "#     # Concatinating the values and reshaping them in\n",
    "#     # (BATCH_SIZE, num_anchors * S * S, 7) shape\n",
    "#     converted_bboxes = torch.cat(\n",
    "#         (best_class, scores, x, y, width_height, angle), dim=-1\n",
    "#     ).reshape(batch_size, num_anchors * s * s, 7)\n",
    "\n",
    "#     # Returning the reshaped and converted bounding box list\n",
    "#     return converted_bboxes.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Getting a sample image from the test data loader \n",
    "try:\n",
    "\tx, y = next(val_loader_iter)\n",
    "except StopIteration:\n",
    "\tval_loader_iter = iter(val_loader)\n",
    "\tx, y = next(val_loader_iter)\n",
    "x = x.to(device) \n",
    "\n",
    "print(\"###################################### display and report image ######################################\")\n",
    "with torch.no_grad():\n",
    "\tscaled_anchors = ( \n",
    "\ttorch.tensor(ANCHORS) *\n",
    "\ttorch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1,3,2) \n",
    "\t).to(device) \n",
    "\toutput = model(x)\n",
    "\ty0, y1, y2 = ( \n",
    "\t\ty[0].to(device), \n",
    "\t\ty[1].to(device), \n",
    "\t\ty[2].to(device), \n",
    "\t) \n",
    "\n",
    "\twith torch.amp.autocast(device_type=device): \n",
    "\t\t# Getting the model predictions \n",
    "\t\toutputs = model(x) \n",
    "\t\t# Calculating the loss at each scale \n",
    "\t\tloss = ( \n",
    "\t\t\tloss_fn(outputs[0], y0, scaled_anchors[0]) \n",
    "\t\t\t+ loss_fn(outputs[1], y1, scaled_anchors[1]) \n",
    "\t\t\t+ loss_fn(outputs[2], y2, scaled_anchors[2]) \n",
    "\t\t) \n",
    "\n",
    "\t# TEMP- print target boxes\n",
    "\tbboxes = [[] for _ in range(x.shape[0])]\n",
    "\tfor i in range(3):\n",
    "\t\tbatch_size, A, S, _, _ = y[i].shape\n",
    "\t\tanchor = scaled_anchors[i]\n",
    "\t\tboxes_scale_i = convert_cells_to_bboxes(y[i], anchor, s=S, is_predictions=False)\n",
    "\t\tfor idx, box in enumerate(boxes_scale_i):\n",
    "\t\t\tbboxes[idx] += box\n",
    "\n",
    "\ti = 0\n",
    "\tprint('bboxes[i] shape:', np.array(bboxes[i]).shape)\n",
    "\tnms_boxes = nms(bboxes[i], iou_threshold=0.5, threshold=0.6)\n",
    "\timg_with_boxes = plot_image(x[i].permute(1, 2, 0).detach().cpu(), nms_boxes, categories)\n",
    "\timg_with_boxes = T.ToTensor()(img_with_boxes)\n",
    "\n",
    "\t# # Print predictions\n",
    "\t# writer.add_scalar('Loss/val', loss.item(), e * len(train_loader) + batch_idx)\n",
    "\n",
    "\t# bboxes = [[] for _ in range(x.shape[0])]\n",
    "\t# for i in range(3):\n",
    "\t# \tbatch_size, A, S, _, _ = output[i].shape\n",
    "\t# \tanchor = scaled_anchors[i]\n",
    "\t# \tboxes_scale_i = convert_cells_to_bboxes(output[i], anchor, s=S, is_predictions=True)\n",
    "\t# \tfor idx, box in enumerate(boxes_scale_i):\n",
    "\t# \t\tbboxes[idx] += box\n",
    "\n",
    "\t# i = 0\n",
    "\t# print('bboxes[i] shape:', np.array(bboxes[i]).shape)\n",
    "\t# nms_boxes = nms(bboxes[i], iou_threshold=0.5, threshold=0.6)\n",
    "\t# img_with_boxes = plot_image(x[i].permute(1, 2, 0).detach().cpu(), nms_boxes, categories)\n",
    "\t# img_with_boxes = T.ToTensor()(img_with_boxes)\n",
    "\t# writer.add_image(f'Val/Image_{e}_{i}_{batch_idx}_before', img_with_boxes, e * len(train_loader) + batch_idx)\n",
    "\n",
    "# model.train()\n",
    "# except Exception as error:\n",
    "# \tprint(error)\n",
    "# \terror_counter += 1\n",
    "# \tif error_counter > 10:\n",
    "# \t\traise error\n",
    "\n",
    "\n",
    "#################\n",
    "# training_loop(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(np.tanh(torch.tensor([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nn.YOLO_VGG16_OBB.utils.helpers import iou\n",
    "\n",
    "def plot_image(image, boxes, labels, display=True):\n",
    "    # Getting the color map from matplotlib\n",
    "    colour_map = plt.get_cmap(\"tab20b\")\n",
    "\n",
    "    # Convert image to NumPy array (if not already)\n",
    "    img = np.array(image)\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    # Copy the image to avoid modifying the original\n",
    "    img_drawn = img.copy()\n",
    "\n",
    "    # Plot bounding boxes and labels\n",
    "    for box in boxes:\n",
    "        class_pred = int(box[0])\n",
    "        cx, cy, bw, bh, angle = box[2:]\n",
    "\n",
    "        # Convert to absolute coordinates\n",
    "        cx, cy, bw, bh = cx * w, cy * h, bw * w, bh * h\n",
    "\n",
    "        # Get color\n",
    "        color = tuple(int(c * 255) for c in colour_map(class_pred + 100)[:3])\n",
    "        if angle > 10:\n",
    "            print(angle)\n",
    "        # Get rotated rectangle\n",
    "        rect = ((cx, cy), (bw, bh), angle)  # OpenCV expects angle in degrees\n",
    "        box_points = cv2.boxPoints(rect)  # Get corner points\n",
    "        box_points = np.int32(box_points)  # Convert to integer\n",
    "\n",
    "        # Draw the rotated rectangle\n",
    "        cv2.polylines(img_drawn, [box_points], isClosed=True, color=color, thickness=1)\n",
    "\n",
    "        # Put label text near the rectangle\n",
    "        # label = labels[class_pred]\n",
    "        # (text_width, text_height), baseline = cv2.getTextSize(\n",
    "        #     label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "        # text_x, text_y = int(cx - text_width / 2), int(cy - bh / 2 - 10)\n",
    "        # cv2.rectangle(img_drawn, (text_x, text_y - text_height - 4),\n",
    "        #               (text_x + text_width, text_y), color, -1)\n",
    "        # cv2.putText(img_drawn, label, (text_x, text_y),\n",
    "        #             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    if display:\n",
    "        # Display the image\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cv2.cvtColor(img_drawn, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return img_drawn  # Return the modified image with drawn bounding boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(bboxes_orig, iou_threshold, threshold):\n",
    "    # Filter out bounding boxes with confidence below the threshold.\n",
    "    bboxes = [box for box in bboxes_orig if box[1] > threshold]\n",
    "\n",
    "    # Sort the bounding boxes by confidence in descending order.\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Initialize the list of bounding boxes after non-maximum suppression.\n",
    "    if (len(bboxes) > 0):\n",
    "        first_box = bboxes.pop(0)\n",
    "        bboxes_nms = [first_box]\n",
    "    else:\n",
    "        bboxes_nms = [max(bboxes_orig, key=lambda x: x[1])]\n",
    "\n",
    "    while len(bboxes) >= 0:\n",
    "        # Iterate over the remaining bounding boxes.\n",
    "        for box in bboxes:\n",
    "            # If the bounding boxes do not overlap or if the first bounding box has\n",
    "            # a higher confidence, then add the second bounding box to the list of\n",
    "            # bounding boxes after non-maximum suppression.\n",
    "            if box[0] != first_box[0] or iou(\n",
    "                    torch.tensor([first_box[2:]]),\n",
    "                    torch.tensor([box[2:]]),\n",
    "            ) < iou_threshold:\n",
    "                # Check if box is not in bboxes_nms\n",
    "                if box not in bboxes_nms:\n",
    "                    # Add box to bboxes_nms\n",
    "                    bboxes_nms.append(box)\n",
    "\n",
    "        # Get the first bounding box.\n",
    "        if len(bboxes) > 0:\n",
    "            first_box = bboxes.pop(0)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Return bounding boxes after non-maximum suppression.\n",
    "    return bboxes_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = target[..., 0] == 1\n",
    "no_obj = target[..., 0] == 0\n",
    "\n",
    "# Calculating No object loss \n",
    "no_object_loss = 4 * bce( \n",
    "    (pred[..., 0:1][no_obj]), (target[..., 0:1][no_obj]), \n",
    ") \n",
    "\n",
    "\n",
    "# Reshaping anchors to match predictions \n",
    "anchors = anchors.reshape(1, 3, 1, 1, 2) \n",
    "# Box prediction confidence \n",
    "box_preds = torch.cat([sigmoid(pred[..., 1:3]), \n",
    "                    torch.exp(pred[..., 3:5]) * anchors, pred[..., 5].unsqueeze(-1)\n",
    "                    ],dim=-1) \n",
    "# Calculating intersection over union for prediction and target \n",
    "ious = iou(box_preds[obj], target[..., 1:6][obj]).detach() \n",
    "ious = ious.unsqueeze(1)\n",
    "object_loss = mse(sigmoid(pred[..., 0:1][obj]), \n",
    "                    ious * target[..., 0:1][obj]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_preds[obj].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_bboxes = torch.cat(\n",
    "    (best_class, scores, x, y, width_height, angle.unsqueeze(0)), dim=-1\n",
    ").reshape(batch_size, num_anchors * s * s, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor.shape\n",
    "anchors = anchor.reshape(1, len(anchor), 1, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_predictions[..., 2:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_predictions = output[i][..., 1:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(box_predictions[..., 2:]) * anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(output[i][..., 2:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd nn/YOLO_VGG16/prepare_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f'../../cocodataset/images/train2017/000000111341.jpg'\n",
    "img = cv2.imread(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nn.YOLO_VGG16_OBB.utils.helpers import iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(bboxes[i]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_orig, iou_threshold, threshold = bboxes[i], 0.5, 0.6\n",
    "# Filter out bounding boxes with confidence below the threshold.\n",
    "bboxes = [box for box in bboxes_orig if box[1] > threshold]\n",
    "\n",
    "# Sort the bounding boxes by confidence in descending order.\n",
    "bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Initialize the list of bounding boxes after non-maximum suppression.\n",
    "if (len(bboxes) > 0):\n",
    "    first_box = bboxes.pop(0)\n",
    "    bboxes_nms = [first_box]\n",
    "else:\n",
    "    bboxes_nms = [max(bboxes_orig, key=lambda x: x[1])]\n",
    "\n",
    "while len(bboxes) >= 0:\n",
    "    # Iterate over the remaining bounding boxes.\n",
    "    for box in bboxes:\n",
    "        # If the bounding boxes do not overlap or if the first bounding box has\n",
    "        # a higher confidence, then add the second bounding box to the list of\n",
    "        # bounding boxes after non-maximum suppression.\n",
    "        if box[0] != first_box[0] or iou(\n",
    "                torch.tensor(first_box[1:]),\n",
    "                torch.tensor(box[1:]),\n",
    "        ) < iou_threshold:\n",
    "            print('**************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obj_d",
   "language": "python",
   "name": "obj_d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
